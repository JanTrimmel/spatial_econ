---
title: "Spatial Economics -- Assignment 3"
author:
  - "Gustav Pirich (h11910449)"
  - "Gabriel Konecny (h11775903)"
date: "May 4, 2024"
output:
  pdf_document:
    toc: true
    includes:
      in_header: !expr file.path("~/Desktop/GITHUB/spatial_econ/helper/wraper_code.tex")
bibliography: references.bib
nocite: '@*'
header-includes:
  - \usepackage{tcolorbox}
  - \usepackage[default]{lato}
  - \usepackage{rotating}
papersize: a4
geometry: margin=2cm
urlcolor: DarkOrchid!65!black
---


```{r, setup, include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(showtext)
showtext_auto()

pacman::p_load(spatialreg, fixest, splm, stringi, stringr, stringdist, haven, sf, dplyr, fuzzyjoin, 
               comparator, digest, zoomerjoin, ggplot2, tidyr, ggthemes, viridis, 
               fixest, conleyreg, plm, stargazer, magrittr, tidyverse, tmap, spdep, SDPDmod,
               igraph, generics, knitr, kableExtra, formatR,readxl, haven, flextable, broom)
```

\vspace{2em}

\begin{tcolorbox}
\centering \itshape The code that was used in compiling the assignment is available on GitHub at \url{https://github.com/gustavpirich/spatial_econ/blob/main/03_assignment/03_assignmnet.Rmd}.
\end{tcolorbox}

\newpage


# Exercise A
```{r, echo=FALSE}
cigar_states <- st_read(file.path("data","cigarettes","cigar_states.xls"))
cigarette_2var <- readxl::read_excel(file.path("data","cigarettes","cigarette+2var.xls")) %>%
                        relocate(state, .before=year) 

test <- cbind(cigar_states, cigarette_2var[,"state"])[,"Name"]

data <- cigarette_2var[c("state", "year", "logc", "logp","logy")] %>%
        mutate(state = cbind(cigar_states, cigarette_2var[,"state"])[,"Name"])
W_vega <- as.matrix(readxl::read_excel(file.path("data","cigarettes","W_vega.xls"), col_names=FALSE), dimnames=FALSE)

W.adj <- as.matrix(readxl::read_excel(file.path("data","cigarettes","Spat-Sym-US.xls"), col_names=FALSE), dimnames=FALSE)
# <- nb2listw(W.adj,style="W")

scale <- function(x){
  x/sum(x)
}
W <- t(apply(W.adj,1,scale))
lW <- mat2listw(W, style="W")
#lW <- mat2listw(W.adj)
```

- Estimate the demand model, and test for spatial dependence using the procedures discussed in class, and the provided weights matrix. Suppress any theoretical considerations
 
The demand model without spatial effects can by estimated by using the package plm:
```{r}
model1 <- plm(logc~logp+logy, data=data, model="within", effect="twoways")
summary(model1)
```
The demand for cigarettes depends negatively on the price of the cigarettes and positively on the real disposable income. 

Alternatively, this could be also done by hand by including the dummies for both fixed effects and running OLS:
```{r}
data_big <- cigarette_2var %>%
            fastDummies::dummy_cols(select_columns = "year") %>%
            fastDummies::dummy_cols(select_columns = "state") %>%
            select(c(logc, logp,logy, starts_with("year_")|starts_with("state_")))
OLS <- lm(logc ~ ., data_big)
OLS
```
Where the point estimates of logp and logy are numerically identical to that of plm(). Thus we can see that using plm with "within" model and "twoways" effect is the way to go.

— which model would the classical specification search prefer?

We run all tests including individual and time fixed effects:
```{r}
fm1 <- logc ~ logp + logy
```


```{r, echo=FALSE}
# Spatial lag structure?
slmtest(fm1, data=data, listw=lW, test="lml", model="within", effect="twoways")

# Spatial error structure?
slmtest(fm1, data=data, listw=lW, test="lme", model="within", effect="twoways")

# Robust versions:
# Spatial lag allowing for spatial error?
slmtest(fm1, data=data, listw=lW, test="rlml", model="within", effect="twoways")

# Spatial error allowing for spatial lag?
slmtest(fm1, data=data, listw=lW, test="rlme", model="within", effect="twoways")
```

Allowing for spatial error, the spatial lag dependence is not significant. Thus this classical specification search suggests estimating SEM model. 

- Estimate the model implied by the specification search and contrast the effect estimates
with a SLX model specification:

Using the spml function, we can estimate SEM model by setting the spatial lag of dependent variable to false (lag=FALSE):
```{r, echo=FALSE}
data <- data.frame(data)
sem <- spml(fm1, data, listw=lW, lag=FALSE, spatial.error = "b", 
                    model="within", effect="twoways") 
```

```{r}
# # `effects()` extracts the individual FE, which are not visible in the `summary()`  
# eff <- effects(sem)
```

SLX model can be estimated by using OLS, i.e. by using plm() function from before where we include the spatially lagged values of explanatory variables:
```{r}

slx_OLS <- function(W){
W_ <- kronecker(diag(nrow(unique(cigarette_2var[,"year"]))), W)
W_logp <- W_ %*% data[,"logp"]
W_logy <- W_ %*% data[,"logy"]
data_slx <- cbind(data, W_logp, W_logy)
slx <- plm(logc~logp+logy+W_logp+W_logy, data=data_slx, model="within", effect="twoways")
return((slx))
}

```

Now, to compare both:
```{r}
summary(sem)
print("SLX Model")
summary(slx_OLS(W))
```

The coefficient estimates above correspond to those reported in table 2 of Vega and Elhorst 2015. Both the SEM and SLX suggest on average a decrease in domestic demand for cigarettes of around 1% following an increase in domestic prices by 1%, ceteris paribus. Similarly both models suggest an increase in demand for cigarettes given an increase in domestic income. SLX coefficient estimate for this is higher. Additionally SLX model provides effects of lagged independ variables of cigarette demand. The SEM model does not provide any additional meaningful parameters since its parameter for spatial autocorrelation of errors is a nuisance parameter.

– What could be the rationale to use the SLX model as opposed to other spatial
specifications?

The choice of the model should be based on theory. If we expect spillovers of exogenous variables and spillovers of endogenous variables or correlation of error terms is expected to be less pronounced or absent, it might be good idea to consider SLX. However, there are also reasons to consider SLX as a point of departure even if the theory is not clear about which model to choose. First, when using the SLX model, the estimation and interpretation of spillover effects is more straightforward. In contrast to endogenous models, SLX allows us to parametrize W and its parameters can be estimated. Strong limitation of SAR and SAC models is that the ratio between the spillover and direct effects of is same for every explanatory variable, which is unlikely to be true in many empirical studies. This is not the case for SLX model, where we get estimates of both by construction. Thus SLX can be considered more flexible in this perspective.

– Which type(s) of spillover(s) would you expect in the model for cigarette demand?

We would expect no endogenous spillover effects, since we dont believe that demand for cigarettes in a country should determine demand for cigarettes in neighboring country if there are no shortages. If however, the price of cigarettes in neighboring country is lower, people living near the border could be buying cigarettes in the neighboring country. This is called bootlegging effect. On other hand, if the income in our country increases, the opportunity costs of time of our citizens increase and they might then prefer to buy the more expensive cigarettes in home country instead. Thus we would expect spatially lagged exogenous variables to be relevant: An increase in price of cigarettes in neighboring country should have positive effect on the demand for cigarettes in home country. Increase in income of neighboring country should have negative effect on the demand for cigarettes in home. 

- Re-run the analysis using a SLX model with a distance decay specification (between centers) for the weights matrix, i.e. wij = 1/dγ ij following Halleck Vega and Elhorst (2015). Use a distance decay parameter of γ = 3.

For this we first create a neighbour list, where all regions are neighbors by setting the distance threshold to big enough value and then compute distances using nbdists. They are then transformed as suggested above and scaled by maximum eigenvalue as in the paper.

```{r}
coords <- cigar_states[,2:3]

decay <- function(gamma){
distw <<- dnearneigh(coords, 0, 999999, row.names=cigar_states$Name, longlat=FALSE)
dists <- nbdists(distw, coords, longlat=FALSE)
ids <- lapply(dists, function(x) lapply(x, function(y) 1/(y^gamma)))
lWd <- nb2listw(distw, glist = ids, style = "B",zero.policy=TRUE)
Wd1 <- listw2mat(lWd)
Wd <- Wd1 / max(abs(eigen(Wd1)$values))
return(Wd)
}
Wd <- decay(3) 
colnames(Wd) <- rownames(Wd)
```

Using gamma similar to the paper, we get coefficient estimates which are very close to those in the paper:
```{r}
summary(slx_OLS(Wd))
```
While the interpretation is non-lagged independent variables is similar as in regressions above, we might want to focus on spatiall lagged variables. The model suggests that increase in price of cigarettes in neighboring regions increases demand for cigarettes in home region, ceteris paribus. This is consistent with the bootlegging behavior discussed in the paper, which predicts that some people might buy cigarettes in/from other regions if the price there is lower. The coefficient estimate for income in neighboring regions suggest lower demand for cigarettes if the real income in neighboring regions  increases. This suggests that if people have higher income their opportunity cost of time is higher and they might prefer the convenience of buying cigarettes in their home region. 

– Describe the network recovered by Halleck Vega and Elhorst (2015):
```{r}
distw
46*46-46
(46*46-46)/46*46
```

In this network all agents are connected, only the weights differ - which are given by the distance decay formula above. This corresponds to average number of links being 45 i.e. all regions except the region itself. Thus the percentage of nonzero weights simply gives the share of diagonal elements of  a 46*46 matrix.

Below we visualize the spatial weights matrix:
```{r, echo=FALSE}
lattice::levelplot((Wd), main="Distance Decay Spatial Weights Matrix",
scales = list(x=list(cex=0.45,rot=90),y=list(cex=.45)))
```
 
Reminding ourself that all regions except the diagonal are neighbors, we can see that the definition of distance decay parameter used results in few strongly connected neighbors and many weaker connections. 

– Who are the central agents and where do spillover effects occur?

We could define the most central agent as one, which has strongest connection to other regions overall. This could correspond to maximizing the nonlinear transformation of distance which we are using. Thus an agent is most central,if the sum of its weights to other agents is highest, i.e. if the sum of its row is highest. 

```{r, echo=FALSE}
head(arrange(data.frame(value = rowSums(Wd)), desc(value)))
```

Maryland is most central agent. It could be that since multiple small regions from around Maryland are included, Maryland has very small distance to them and thus its inverse is large, contributing a lot to maximization of criterion made up above. Thus most central agent is one which has many close neighbors. From the table above we see that other important agents are District of Columbia, Massachusetts and Rhode Island.

We get similar results when using eigenvector centrality:

```{r, echo=FALSE}
G <- graph_from_adjacency_matrix(
  Wd,
  mode = c("undirected"),
  weighted = TRUE,
)

ec <- data.frame(state=cigar_states$Name, ec=eigen_centrality(G)$vector) %>%
      arrange(desc(ec))

knitr::kable(ec, caption = "Eigenvector Centrality")

```

– What is the average partial effect of increasing income?
We compute APE as follows:
$\beta_{logy} + \theta_{logy} \frac{1}{N} \sum_{j}^{J}   \sum_{i}^{I}  w_{ij} $
where I is number of rows of W and J number of columns. 
```{r}
beta <- coef(slx_OLS(Wd))[["logy"]]
theta <- coef(slx_OLS(Wd))[["W_logy"]]
N <- nrow(Wd)*ncol(Wd)
beta + theta*sum(Wd)/N
```
Thus, the average partial effect of increasing income of is very close to the effect of increasing income in home only. This is because using the specification of distance decay above, the links between the agents are mostly weak.


# Exercise B
```{r, echo = FALSE}
setwd("~/Desktop/GITHUB/spatial_econ/data/03_assignment/dataset")

raster_Africa <- read_sf("~/Desktop/GITHUB/spatial_econ/data/03_assignment/dataset/raster_Africa.shp")

geoconflict_main <- read_dta("~/Desktop/GITHUB/spatial_econ/data/03_assignment/dataset/geoconflict_main.dta")

intersect_coord <- read_dta("~/Desktop/GITHUB/spatial_econ/data/03_assignment/dataset/intersect_coord.dta")
```

## Unit of Observation
```{r, echo = FALSE}
raster_Africa <- read_sf("~/Desktop/GITHUB/spatial_econ/data/03_assignment/dataset/raster_Africa.shp")

area <- st_area(raster_Africa)

raster_Africa %<>%
  mutate(area = st_area(geometry))

data <- data.frame(Cells = c("Min", "Mean", "Median", "Max"), Stats = c(min(area), mean(area), median(area), max(area)))

kable(data)
```
The unit of observation are cells in a raster representing a 1×1 degree latitude longitude grid cover. At the equator this corresponds to a side length of 110 km. The areal extension of the cells varies with latitude. Further away from the equator, the area of a cell decreases. 
The table shows the area of the cells. The smallest cell has an area of about 97 857 $km^{2}$, the largest 12 364 $km^{2}$. On average the size of the cells is 11 900 $km^{2}$.  Thus the gap between the smallest and largest cell amounts to about 20%.
They differ because of the distortions induced by the projection of the surface of the earth, onto a 2 dimensional raster grid. 

## Interpretation of Coefficients

## Replication

```{r}
# preparation of dataset --------------------------------------------------

# exclude year 1997 to obtain same number of observations
geoconflict_main_if <- geoconflict_main %>% 
  filter(!year == 1997)

# format as factor 
geoconflict_main_if$year <- as.factor(geoconflict_main_if$year)
geoconflict_main_if$cell <- as.factor(geoconflict_main_if$cell)


################
# 1st Column OLS

fml1 = ANY_EVENT_ACLED ~ SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + elevation_cell + rough_cell + area_cell + as.factor(use_primary) + 
  dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral) + ELF + i(country_largest_share, as.numeric(year), ref = "Zimbabwe") | as.factor(year)

mod1 <- feols(fml1, 
              data = geoconflict_main_if, 
              panel.id = c('cell', 'year'))

################


################
# 2nd Column OLS - missing spatially lagged linear time trends - also column 2 in the table looks like there are some coefficients/standard errors missing? 

fml2 = ANY_EVENT_ACLED ~ SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + W_L2_GSmain_ext_SPEI4pg + W_L1_GSmain_ext_SPEI4pg + W_GSmain_ext_SPEI4pg + W_SPEI4pg + W_L1_SPEI4pg + W_L2_SPEI4pg + elevation_cell + 
  rough_cell + area_cell + as.factor(use_primary) + 
  dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral)+ ELF + W_elevation_cell + W_rough_cell + W_area_cell + W_ELF + as.factor(W_any_mineral) + as.factor(W_shared)  + 
  W_dis_river_cell + as.factor(W_use_primary) + i(as.factor(country_largest_share), as.numeric(year), ref = "Zimbabwe") | as.factor(year)

mod2 <- feols(fml2, 
              data = geoconflict_main_if, 
              panel.id = c('cell', 'year'))

################

etable(mod1, mod2, 
       keep = c("SPEI4pg", "L1_SPEI4pg", "L2_SPEI4pg", "GSmain_ext_SPEI4pg", "L1_GSmain_ext_SPEI4pg", "L2_GSmain_ext_SPEI4pg", "W_SPEI4pg",  "W_L1_SPEI4pg", "W_L2_SPEI4pg", "W_GSmain_ext_SPEI4pg", "W_L1_GSmain_ext_SPEI4pg", "W_L2_GSmain_ext_SPEI4pg"), tex = TRUE)
```


```{r}
################
# 3rd Column  -----------------------------------------------------

# generate spatial weights matrix -----------------------------------------
geoconflict_main_weights <- geoconflict_main_if %>%
  st_as_sf(coords = c("lat", "lon"), crs = "WGS84") %>%
  filter(year == 2011) # could be any year

# setting the distance threshold to 180 kilometers
neighboors <- dnearneigh(st_centroid(geoconflict_main_weights), d1 = 0, d2 = 180, use_s2 = TRUE)

# create binary spatial weights matrix
listwmat <- nb2listw(neighboors, style ="B", zero.policy = TRUE)

# specification 3 - we might be missing the country specific linear time trends of the neighbors 
mod3 <- spml(ANY_EVENT_ACLED ~ lag(ANY_EVENT_ACLED) + SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + W_GSmain_ext_SPEI4pg +
                               W_L1_GSmain_ext_SPEI4pg + W_L2_GSmain_ext_SPEI4pg + W_SPEI4pg + W_L1_SPEI4pg + W_L2_SPEI4pg + elevation_cell + 
                               rough_cell + area_cell + as.factor(use_primary) + dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral) +
                               ELF + W_elevation_cell + W_rough_cell + W_area_cell + W_ELF + as.factor(W_any_mineral) + as.factor(W_shared)  + 
                               W_dis_river_cell + as.factor(W_use_primary) + as.factor(country_largest_share):as.numeric(year), # country specific linear time trends
                             data = as.data.frame(geoconflict_main_if), 
                             index=c("cell","year"),
                             listw = listwmat,
                             model="pooling",
                             effect = "time",  
                             dynamic = TRUE,
                             spatial.error="none",
                             zero.policy = TRUE, 
                             lag=TRUE)

################



################
# 4th column 

# Preparation adding column for country x year fixed effects (country_year_fe)
geoconflict_main_fe <- geoconflict_main_if %>% 
  mutate(country_year_fe = paste0(country_largest_share, year))

mod4 <- spml(ANY_EVENT_ACLED ~ lag(ANY_EVENT_ACLED) + SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + W_GSmain_ext_SPEI4pg + W_L1_GSmain_ext_SPEI4pg + W_L2_GSmain_ext_SPEI4pg + W_SPEI4pg + W_L1_SPEI4pg + W_L2_SPEI4pg + elevation_cell + rough_cell + area_cell + as.factor(use_primary) + dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral) +
                              ELF + W_elevation_cell + W_rough_cell + W_area_cell + W_ELF + as.factor(W_any_mineral) + as.factor(W_shared)  + 
                              W_dis_river_cell + as.factor(W_use_primary) + as.factor(country_year_fe), # country x year FE
                             data = as.data.frame(geoconflict_main_fe), 
                             index= c("cell","year"),
                             listw = listwmat,
                             model = "pooling",  # no fixed effects
                             effect = "individual", 
                             spatial.error="none",
                             zero.policy = TRUE, 
                             dynamic = TRUE,
                             lag = TRUE, 
                             Hess = TRUE,
                             local=list( parallel = T)) # makes it faster

summary(mod3)
summary(mod4)

```

We now create a horizontal adjacency matrix based on horizontal contiguity.

```{r}
### Lets first create teh bianry contiguity matrix based on horizontal contiguity 
# Extract centroids of the polygons if not already point data
geoconflict_main_weights

# Calculate distances and filter horizontally contiguous points
# Define a function to find neighbors based on conditions
find_neighbors_horizontal <- function(centroids, max_distance = 150000, lat_tolerance = 0.01) {
  # Create a matrix to store distances
  distances <- st_distance(centroids)
  latitudes <- st_coordinates(centroids)[,2] # Extract latitudes
  
  # Initialize neighbors list
  neighbors_list <- vector("list", nrow(centroids))
  
  for (i in seq_len(nrow(centroids))) {
    # Find points within the same latitude band and within distance
    lat_condition <- abs(latitudes - latitudes[i]) < lat_tolerance
    dist_condition <- distances[i,] < set_units(max_distance, "meters")
    
    # Combine conditions
    neighbor_ids <- which(lat_condition & dist_condition)
    
    # Exclude the point itself from its list of neighbors
    neighbor_ids <- neighbor_ids[neighbor_ids != i]
    
    # Store neighbor ids
    neighbors_list[[i]] <- as.integer(neighbor_ids)
  }
  
  return(neighbors_list)
}

find_neighbors_vertical <- function(centroids, max_distance = 150000, lon_tolerance = 0.01) {
  # Create a matrix to store distances
  distances <- st_distance(centroids)
  longitudes <- st_coordinates(centroids)[,1] # Extract longitudes
  
  # Initialize neighbors list
  neighbors_list <- vector("list", nrow(centroids))
  
  for (i in seq_len(nrow(centroids))) {
    # Find points within the same longitude band and within distance
    lon_condition <- abs(longitudes - longitudes[i]) < lon_tolerance
    dist_condition <- distances[i,] < set_units(max_distance, "meters")
    
    # Combine conditions
    neighbor_ids <- which(lon_condition & dist_condition)
    
    # Exclude the point itself from its list of neighbors
    neighbor_ids <- neighbor_ids[neighbor_ids != i]
    
    # Store neighbor ids
    neighbors_list[[i]] <- as.integer(neighbor_ids)
  }
  
  return(neighbors_list)
}

# Apply the function
neighbors_vertical <- find_neighbors_vertical(geoconflict_main_weights)

neighbors_horizontal <- find_neighbors_horizontal(geoconflict_main_weights)



# Number of elements
n <- length(neighbors_vertical)

# Initialize an adjacency matrix with 0s
adj_matrix_vertical <- matrix(0, nrow = n, ncol = n)
adj_matrix_horizontal <- matrix(0, nrow = n, ncol = n)

# Populate the adjacency matrix
for (i in seq_len(n)) {
  # Ensure indices are within the valid range
  valid_indices_vertical <- neighbors_vertical[[i]][neighbors_vertical[[i]] <= n]
  valid_indices_horizontal <- neighbors_vertical[[i]][neighbors_vertical[[i]] <= n]

  # Set matrix elements to 1
  adj_matrix_vertical[i, valid_indices_vertical] <- 1
  adj_matrix_horizontal[i, valid_indices_horizontal] <- 1
}

# Turn the matrices into listw objects
vertical_listw <- mat2listw(adj_matrix_vertical, style = "B", zero.policy=TRUE)
horizontal_listw <-mat2listw(adj_matrix_horizontal, style = "B", zero.policy=TRUE)
```
[[677]]
Simple feature collection with 3 features and 166 fields
Geometry type: POINT
We can now rerun our analysis
```{r, echo = FALSE}
modvertical3 <- spml(ANY_EVENT_ACLED ~ lag(ANY_EVENT_ACLED) + SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + W_GSmain_ext_SPEI4pg + W_L1_GSmain_ext_SPEI4pg + W_L2_GSmain_ext_SPEI4pg + W_SPEI4pg + W_L1_SPEI4pg + W_L2_SPEI4pg + elevation_cell + rough_cell + area_cell + as.factor(use_primary) + dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral) + ELF + W_elevation_cell + W_rough_cell + W_area_cell + W_ELF + as.factor(W_any_mineral) + as.factor(W_shared)  + W_dis_river_cell + as.factor(W_use_primary) + as.factor(country_largest_share):as.numeric(year), # country specific linear time trends
                             data = as.data.frame(geoconflict_main_if), 
                             index=c("cell","year"),
                             listw = vertical_listw,
                             model="pooling",
                             effect = "time",  
                             dynamic = TRUE,
                             spatial.error="none",
                             zero.policy = TRUE, 
                             lag=TRUE,
                             local=list( parallel = T))

modelhorizontal3 <- spml(ANY_EVENT_ACLED ~ lag(ANY_EVENT_ACLED) + SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + W_GSmain_ext_SPEI4pg + W_L1_GSmain_ext_SPEI4pg + W_L2_GSmain_ext_SPEI4pg + W_SPEI4pg + W_L1_SPEI4pg + W_L2_SPEI4pg + elevation_cell + rough_cell + area_cell + as.factor(use_primary) + dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral) + ELF + W_elevation_cell + W_rough_cell + W_area_cell + W_ELF + as.factor(W_any_mineral) + as.factor(W_shared)  + W_dis_river_cell + as.factor(W_use_primary) + as.factor(country_largest_share):as.numeric(year), # country specific linear time trends
                             data = as.data.frame(geoconflict_main_if), 
                             index=c("cell","year"),
                             listw = horizontal_listw,
                             model="pooling",
                             effect = "time",  
                             dynamic = TRUE,
                             spatial.error="none",
                             zero.policy = TRUE, 
                             lag=TRUE,
                             local=list(parallel = T))

modelhorizontal4 <- spml(ANY_EVENT_ACLED ~ lag(ANY_EVENT_ACLED) + SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + W_GSmain_ext_SPEI4pg + W_L1_GSmain_ext_SPEI4pg + W_L2_GSmain_ext_SPEI4pg + W_SPEI4pg + W_L1_SPEI4pg + W_L2_SPEI4pg + elevation_cell + rough_cell + area_cell + as.factor(use_primary) + dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral) +
                              ELF + W_elevation_cell + W_rough_cell + W_area_cell + W_ELF + as.factor(W_any_mineral) + as.factor(W_shared)  + 
                              W_dis_river_cell + as.factor(W_use_primary) + as.factor(country_year_fe), # country x year FE
                             data = as.data.frame(geoconflict_main_fe), 
                             index= c("cell","year"),
                             listw = horizontal_listw,
                             model = "pooling",  # no fixed effects
                             effect = "individual", 
                             spatial.error="none",
                             zero.policy = TRUE, 
                             dynamic = TRUE,
                             lag = TRUE, 
                             Hess = TRUE,
                             local=list( parallel = T)) # makes it faster

modelvertical4 <- spml(ANY_EVENT_ACLED ~ lag(ANY_EVENT_ACLED) + SPEI4pg + L1_SPEI4pg + L2_SPEI4pg + GSmain_ext_SPEI4pg + L1_GSmain_ext_SPEI4pg + L2_GSmain_ext_SPEI4pg + W_GSmain_ext_SPEI4pg + W_L1_GSmain_ext_SPEI4pg + W_L2_GSmain_ext_SPEI4pg + W_SPEI4pg + W_L1_SPEI4pg + W_L2_SPEI4pg + elevation_cell + rough_cell + area_cell + as.factor(use_primary) + dis_river_cell + as.factor(shared) +  as.factor(border) + as.factor(any_mineral) +
                              ELF + W_elevation_cell + W_rough_cell + W_area_cell + W_ELF + as.factor(W_any_mineral) + as.factor(W_shared)  + 
                              W_dis_river_cell + as.factor(W_use_primary) + as.factor(country_year_fe), # country x year FE
                             data = as.data.frame(geoconflict_main_fe), 
                             index= c("cell","year"),
                             listw = vertical_listw,
                             model = "pooling",  # no fixed effects
                             effect = "individual", 
                             spatial.error="none",
                             zero.policy = TRUE, 
                             dynamic = TRUE,
                             lag = TRUE, 
                             Hess = TRUE,
                             local=list( parallel = T)) # makes it faster

```









## Summarize and visualize the weights matrixes and briefly explain what they imply

The horizontal contiguity matrix means that only cells which share the same latitude are considered to be adjacent. Vertical contiguity means that only cells which share the same longitude are considered adjacent. It is hard to make a coherent case for why this should be that way in the real world. There is no inherent reason to assume that effects only propagate in one 'direction'. 

The horizontal contiguity matrix has an average number of paths 

```{r}
adj_matrix_horizontal
adj_matrix_vertical

graph_dist <- graph_from_adjacency_matrix(adj_matrix_vertical, mode = "undirected")

# Function to summarize graph properties and return as a data frame
summarize_graph <- function(g) {
  # Function to format the number based on its value
  format_number <- function(x) {
    if (floor(x) == x) {  # If the number is an integer
      return(as.character(x))  # Return without decimal places
    } else {
      return(format(x, nsmall = 2))  # Return with two decimal places
    }
  }
  
  # Apply the format_number function to each relevant graph property
  graph_summary <- data.frame(
    Property = c("Number of vertices", "Number of edges", "Average path length", 
                 "Graph density", "Average degree", "Max Eigenvector Centrality", 
                 "Min Eigenvector Centrality", "Average Eigenvector Centrality", 
                 "Most Central Unit (Vertex ID)"),
    Value = sapply(c(vcount(g), ecount(g), 
              average.path.length(g, directed = FALSE), 
              edge_density(g), 
              mean(degree(g)),
              max(eigen_centrality(g)$vector),
              min(eigen_centrality(g)$vector),
              mean(eigen_centrality(g)$vector),
              as.numeric(V(g)[which.max(eigen_centrality(g)$vector)])), format_number)
  )
  
  return(graph_summary)
}


summarize_graph(graph_dist)


raster::image((adj_matrix_vertical), main="Smooth Distance-Decay Spatial Weights Matrix")

```
