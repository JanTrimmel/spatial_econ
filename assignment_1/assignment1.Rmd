---
title: "**Spatial Economics -- Assignment 1**"
author:
- Gustav Pirich (h11742049@s.wu.ac.at)
- Gabriel Konecny (h11775903@s.wu.ac.at)
- Jan Trimmel (h11809096@s.wu.ac.at)
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    includes:
      in_header: null
  html_document:
    toc: true
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{arev}
- \usepackage[T1]{fontenc}
- \usepackage{setspace}\onehalfspace
papersize: a4
geometry: margin = 2cm
urlcolor: DarkOrchid!65!black
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("spDataLarge", repos = "https://geocompr.r-universe.dev")

library(igraph)
library(sf)
library(ggplot2)
library(tmap)
library(tidyverse)
library(spDataLarge)
library(RColorBrewer)
library(gt)
library(sf)
library(readxl)
library(colorRamps)
library(viridis)

```

# Exercise A
For our independent variables, we use per capita crime rate by town, average number of rooms per dwelling, 
Charles River dummy variable (= 1 if tract bounds river; 0 otherwise), 
nitrogen oxides concentration (parts per 10 million) and a constant.

```{r, echo=FALSE}
data1 <- MASS::Boston
X <- as.matrix(cbind(1,data1[,c("crim", "rm","chas","nox")]))
#X <- as.matrix(cbind(1,data1["lstat"]))
colnames(X) <- c("Constant","Crime", "Rooms", "Charles_River", "NO_pp10m")
Y <- as.matrix(data1[,"medv"])
```

Below we wrote a function which computes:
OLS point estimates for the intercept, slope parameters, and the error variance.
Suitable test statistics with corresponding p-values for the relevant coefficients.
Intervals of the coefficients for a confidence level of 95\%.

```{r}
OLS <- function(X,Y){

# OLS estimates for coefficients
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y  
Y_hat <- X %*% beta_hat   # Fitted values

e <- Y - Y_hat            # residuals
n <- nrow(X)              # Number of observations
k <- ncol(X) - 1          # Number of covariates excluding intercept

s <- as.numeric(t(e)%*%e / (n-k))           # SSR adjusted for degrees of freedom
sigma <- s*solve(t(X) %*% X)                # VCV of Beta hat

se <- sqrt(diag(sigma))                     # standard error
t_stat <- (beta_hat-0) / se                 # Compute t-statistic
p <- pt(abs(t_stat), n-k, lower.tail=FALSE) # Compute p-value

# 95% Confidence interval
th <- qt(0.975, n-k)
conf <- cbind(beta_hat-th*se,beta_hat+th*se) 

colnames(beta_hat) <- "estimate"
colnames(conf) <- c("2.5%","97.5%")
colnames(t_stat) <- "t-statistic"
colnames(p) <- "p-value"

error_variance <- s

list(rbind(beta_hat,error_variance), cbind(t_stat, p), conf)
} 
```

The output of the function is presented below:
```{r}
OLS(X,Y)
```

# Exercise B
1. Draw a graph of the network; create the adjacency matrix in R.

```{r, echo = FALSE}
agents <- c("US", "MX", "DE", "AT", "CN", "IN")

adj <- matrix(c(
    0, 1, 0, 0, 1, 0,
    1, 0, 0, 0, 1, 0,
    1, 0, 0, 0, 0, 0,
    1, 0, 1, 0, 1, 0,
    1, 0, 1, 0, 0, 0,
    1, 0, 0, 0, 1, 0
), nrow = 6, byrow = TRUE)

colnames(adj) = rownames(adj) = agents
```

We could consider 6 different countries and an indicator of a high trade between them. For illustration, we could consider a rule where there is directed edge from A to B, if B is one of the top 5 export destinations of A. We don't actually check for biggest trading partners empirically, but lets assume that such procedure gives rise to following network (arrows are mostly made up to achieve 12 edges and nice interpretations):

```{r, echo=FALSE}
plot(graph_from_adjacency_matrix(adj),vertex.size = 30, main="Trade network",edge.arrow.size = 0.7)
```

The corresponding adjacency matrix is:

```{r, echo=FALSE}
print(adj)
```
2. Who are the most and least central agents in the network? Name, explain, and try to quantify different notions of centrality.

A very basic concept of centrality could define an agent as most central, if it has the highest number of directed edges pointing towards itself (i.e. if it is important export country for most other countries). Using this criterion we can see from the graph that or from columns of adjacency matrix that US the most central agents in this network, because it is top 5 trading partner for all 5 other countries. The least central agents would be Austria and India, since they are not a top 5 export country for any country from this network.

Another basic criterion of centrality could be the amount of outwards pointing arrows of an agent. Thus a country would be considered central if it exports to highest number of countries from this network. In this sense, Germany is least central with only 1 outward arrow, while Austria is most central with 3 outward arrows.

Eigenvector centrality: There is no sink, but not possible to get to Austria. ?
Page Rank: weights (probs) need to be defined?

- How would centralities change if you considered a row-normalized network
instead?


```{r, echo=FALSE}
scale <- function(x){
  x/sum(x)
}
adj.rn <- t(apply(adj,1,scale))
```

Germany which had only 1 connection, is the only row with sum of 1. Thus its the only edge which survives row normalization. Thus here depending on criterion US or Germany would be most central agent. Other countries would be all least central.

```{r, echo=FALSE}
plot(graph_from_adjacency_matrix(adj.rn),vertex.size = 30, main="Trade network row normalized",edge.arrow.size = 0.7)
```

- How would the network change if you removed or added a specific agent?
Lets consider the case of removing US:

```{r, echo=FALSE}
#agents[-1]
plot(graph_from_adjacency_matrix(adj[-1,-1]),vertex.size = 30, main="Trade network excl. US",edge.arrow.size = 0.7)
```

Based on Inflowing arrows, China becomes the most central agent and India, Austria and Mexico the least central agents. 
Based on Outflowing arrows, Austria is still the most central agent, while Germany is the least central agent.

3. Simulate some agent characteristic based on a standard Normal distribution; use this characteristic to simulate responses following a liner-in-means model with your network. Repeat this a couple of times, and compare estimates of a standard linear model ($y_{i} = x_{\beta} + \varepsilon_{i}$) with the true values that you used to simulate the data

We simulate the data based on the row normalized adjacency matrix for trade partner connections, that we created in the preceding exercise.

Let's say we are interested in estimating the impact of a countries fentanyl precursor production (x) on the log of a countries drug deaths (y). Then Wx denotes the average of neighboring countries fentanyl precursor production. We hypothesize that if a countries close trade partners fentanyl precursor production rate is high (as measured by Wx), then the fentanyl related drug deaths are high. Moreover, there are spillover effects with regards to the trading partners fentanyls related deaths from connected countries (Wy).  

We specify the following linear-in-means model to estimate the relationship.    
$$y = Wy \lambda + Wx \delta + x\beta + \varepsilon$$

Where $Wy$ denotes the average death rate from drugs of countries that are close neighbors.   

To simulate the values for y, we need to solve for the reduced form. 
$$y = (I - W\lambda)^{-1}(Wx \delta + x\beta + \varepsilon)$$
```{r}
set.seed(123)
# number of agents
N = 6

# parameters definition
sigma2 = 1
lambda = 0.6
delta = 0.3
beta = 2 # true beta is 2
W = adj.rn

simulations <- 10000
times <- c(1:simulations)
result <- numeric(simulations) 

for (i in times) {

# simulation of vecotrs
x = rnorm(N, 0, 1)
e = rnorm(N, 0, sigma2)

# caluclating means
Wx <- W %*% x

# calculating S
S = diag(N) - lambda * W

# generating y's
y = solve(S, Wx * delta + x * beta + e)
  
model <- lm(y ~ x)

result[i] <- coef(model)["x"]
} 

inconsistent_estimate <- mean(result)

my_data <- data.frame(
  Name =  c("Simulated coef", "Real coef"), 
  Estimate = c(inconsistent_estimate, beta) 
)

knitr::kable(my_data, format = "markdown")
```

We set the true value for $\beta$ to 2, $\lambda$ to 0.6, and $\delta$ to 0.3. We then regress y on x, repeat this a 1000 times, and find that the mean of the estimand is 1.718638. Thus using the simple linear regression fails to recover the true value of $\beta$. 

# Exercise C
Download a suitable shapefile for NUTS2 regions (from here or using R directly) and some
dataset of interest at the same level of aggregation (e.g. from here). Install and load the
sf and ggplot2 packages together with their dependencies.

1. Read in the shapefile, and find out what projection and CRS the file uses. Map the data to use another projection and/or CRS of your choosing:

```{r, echo = FALSE}
shp <- st_read(dsn = "./data", layer ="NUTS_RG_03M_2021_4326_LEVL_2") 

old_crs <- st_crs(shp)

print(old_crs$input)

shp_27700 <- st_transform(shp, 27700)

new_crs <- st_crs(shp_27700)

print(new_crs$input)
```

The projection used is the "World Geodetic System 1984" or for short "WGS 84" (EPSG 4326). We then map the data to the "North American Datum 1983" (NAD83) projection, which is extremely similar to the WGS 84. 


2. Merge the shapefile and the chosen dataset Create tow meaningful visualizations of the chosen dataset using different scales (e.g. continuous versus discrete scaling of the data)

```{r, include = FALSE, warning=FALSE}
agr_r_accts_page_spreadsheet <- read_excel("/Users/gustavpirich/Desktop/GITHUB/spatial_econ/data/agriculture_NUTS2_2022/agr_r_accts_page_spreadsheet.xlsx", sheet = 3, range = "A11:C472")

agr_r_accts_page_spreadsheet_1 <- agr_r_accts_page_spreadsheet %>%
  rename("NUTS_ID" = "GEO (Codes)") %>%
  rename("ag_value_added_1" = "...3") %>%
  mutate(ag_value_added_1 = as.numeric(ag_value_added_1))

# Remove commas and convert periods to decimal points if they are used as such
#agr_r_accts_page_spreadsheet_1$ag_value_added_1 <- gsub(",", "", agr_r_accts_page_spreadsheet_1$ag_value_added_1) # Remove commas

shp_merged <- left_join(shp, agr_r_accts_page_spreadsheet_1)
```

```{r, echo = FALSE, warning=FALSE}
#excluding overseas regions
overseas <- c("FRY1", "FRY2", "FRY3", "FRY4", "FRY5", "FRZZ", 
              "PT20", "PT30", "PTZZ", 
              "ES70", "ESZZ", 
              "NO0B", "NOZZ")

shp_merged_filter <- shp_merged %>%
  filter(!NUTS_ID %in% overseas) %>%
  filter(!is.na(ag_value_added_1))
```

```{r, echo = FALSE, warning=FALSE, fig.align='center'}
# Your existing code for plotting
ggplot(data = shp_merged_filter) +
  geom_sf(aes(fill = ag_value_added_1)) + 
  scale_fill_viridis_c(name = "Millions EUR",
                       option = "D") + # 'option' can be "C" for the default viridis palette, or "A", "B", "D", "E" for other variants +
  labs(title = "Agricultural Value Added by NUTS-2 Region in 2021") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(legend.position = "bottom",
        legend.text = element_text(size = 8), # Decrease text size
        legend.title = element_text(size = 9), # Decrease title size
        legend.key.size = unit(0.5, "cm")) # Decrease key size (you can adjust the value as needed)

# Define the breaks for the bins   scale_fill_gradientn(colors = c("lightblue2", "turquoise4", "navyblue"), 
                       #values = scales::rescale(c(min(shp_merged_filter$ag_value_added_1, na.rm = TRUE), 
                        #                          mean(shp_merged_filter$ag_value_added_1, na.rm = TRUE), 
                         #                         max(shp_merged_filter$ag_value_added_1, na.rm = TRUE))),

breaks <- seq(0, 15000, by = 1000)

# Create labels for the bins based on the breaks
labels <- paste(head(breaks, -1), tail(breaks, -1) - 1, sep = "-")
```

```{r, echo = FALSE, fig.align='center'}
# Discretize the share variable into bins
shp_merged_filter_1 <- shp_merged_filter %>%
  mutate(share_bins = cut(ag_value_added_1,
                          breaks = breaks,
                          include.lowest = TRUE,
                          labels = labels)) %>%
  filter(!is.na(share_bins))


# Plot the map with the discretized bins using the viridis color palette
ggplot(data = shp_merged_filter_1) +
  geom_sf(aes(fill = share_bins)) +
  scale_fill_viridis_d(name = "Millions EUR", option = "D") + # 'option' can be "C" for the default viridis palette, or "A", "B", "D", "E" for other variants
  labs(title = "Binned Agricultural Value Added per NUTS-2 Region in 2021") +
  theme_minimal() +
  theme(legend.position = "bottom",
        legend.text = element_text(size = 5), # Decrease text size
        legend.title = element_text(size = 6), # Decrease title size
        legend.key.size = unit(0.5, "cm")) # Decrease key size (you can adjust the value as needed)
```

The discrete scaling shows the agricultural value added in million Euros at a NUTS-2 Level in 2021. While the first map visualizes the value added in continous scaling. The second map portrays agricultral value added in a binned scaling. Note that animal production and fishing is also included, which accounts for the high agrilcutlral value added in Western France.  


3. Briefly explain what two conceptually different ways there are to store visualizations, and name two formats each. Which ones are more appropriate for which types of visualizations and why? Use the one you think is more appropriate to store your visualizations.

There are two types of spatial data, raster and vector data. Raster graphics can be stored as a png and jpeg. Vector graphics can be stored as a svg (Scalable Vector Graphics) or eps (Encapsulated PostScript) file. Vector graphics use mathematical equations like shapes, colours, and lines. An advanatage of vector graphics is that they do not lose resolution when you zoom in on them. 

# Exercise D

We start by visualizing the results of the second round of the Polish election. The first map displays the winner of the respecetive district. Overall we can see that the election was extremely close, with Duda winning just 51.55% and Komorowski winning  48.45% of the popular vote. One can clearly observe an east west pattern, where the Western part of the country predominantly voted for Duda and likewise the eastern part of the country for Komrowski.

```{r pol_pres15$Support, echo=FALSE, fig.align='center'}
# Visualization comparing support for Komorowski and Duda
pol_pres15$Support <- ifelse(pol_pres15$I_Komorowski_share > pol_pres15$I_Duda_share, "Komorowski (48.45%)", "Duda (51.55%)")

# Visualization comparing support for Komorowski and Duda in the first round
tm_shape(pol_pres15) + 
  tm_fill(col = "Support", style = "cat", palette = "RdYlBu", 
          title = "Majority for Candidate in Second Round") +
  tm_layout(legend.position = c("left", "bottom"))
```

```{r, echo = FALSE}
komorowskiPalette <- brewer.pal(n = 9, name = "Blues")
dudaPalette <- brewer.pal(n = 9, name = "Reds")

data <- pol_pres15 %>% 
  mutate(II_Komorowski_share = 100*II_Komorowski_share) %>%
  mutate(II_Duda_share = 100*II_Duda_share) 
```

We now zoom in on the shares by using the function tm_facets, which allows to display the vote shares of the candidates separatley. The maps now display the vote shares of the respective candidates in each district. We can observe a strong polarization, as a large number of districts voted by above 80% for the respective candidate.
```{r, echo = FALSE, fig.width= 12}
tm_shape(data) +
  tm_facets(free.scales = TRUE) +  # Enables independent scaling of facets
  tm_fill(c("II_Komorowski_share", "II_Duda_share"), title = "2. Round Vote Share",
              palette=list("II_Komorowski_share" = komorowskiPalette,
                           "II_Duda_share" = dudaPalette),
              midpoint=50) + 
  #tm_borders(lwd = 0.2, alpha = 0.7) +
  tm_layout(legend.position = c("left", "bottom"), scale = 0.6)
```


## Possible Issues with voting envelopes

The next map is showing possible issues with voting envelopes. Specifically, we wanted to investigate if all envelopes sent were also received. If the number of envelopes received is equal to the number of the envelopes sent, the municipality is visualized green. If the municipality is visualized in grey, that means that there were no voting envelopes used for the election. If a municipality is red however, that indicates that the total number of envelopes sent is higher than the number of envelopes received (at a threshold off 99 percent). That indicates that there were probably issues with the delievery of the voting envelopes. We can observe red clusters especially in urban regions (Warsaw, Katowice and Krakow). An interesting side fact however is that one can clearly see that in urban areas, voting envelopes were used more often than in rural areas of Poland:

```{r pol_pres15$Envelope_Proportion, echo=FALSE}
# Calculate the proportion of postal voting envelopes received to postal voting packages sent
pol_pres15$Envelope_Proportion <- pol_pres15$I_postal_voting_envelopes_received / pol_pres15$I_voters_sent_postal_voting_package
pol_pres15$II_Envelope_Proportion <- pol_pres15$II_postal_voting_envelopes_received / pol_pres15$II_voters_sent_postal_voting_package

pol_pres15$II_Envelope_Proportion <- ifelse(is.na(pol_pres15$II_Envelope_Proportion), 1, pol_pres15$II_Envelope_Proportion)

# Define a threshold for the proportion indicating possible issues
threshold <- 0.99

# Create a new column indicating possible issues with postal voting envelopes based on the proportion
pol_pres15$Envelope_Issues <- ifelse(pol_pres15$II_Envelope_Proportion < threshold, "Possible Issues", "No Issues")

# Visualization investigating possible issues with postal voting envelopes
tm_shape(pol_pres15) + 
  tm_fill(col = "Envelope_Issues", palette = c("blue", "orange"),
          title = "Postal voting Received") +
  tm_layout(legend.position = c("left", "bottom"))
```





