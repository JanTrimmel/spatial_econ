---
title: '**Spatial Economics -- Assignment 1**'
author:
- Gustav Pirich (h11742049@s.wu.ac.at)
- Gabriel Konecny (h11775903@s.wu.ac.at)
- Jan Trimmel (h11809096@s.wu.ac.at)
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: no
  html_document:
    toc: no
    df_print: paged
header-includes:
- \usepackage{tcolorbox}
- \usepackage{bm}
papersize: a4
geometry: margin = 2cm
urlcolor: Mahogany
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("spDataLarge", repos = "https://geocompr.r-universe.dev")

library(igraph)
library(tmap)
library(tidyverse)
library(spDataLarge)
library(RColorBrewer)

```

## Exercise A
For our independent variables, we use per capita crime rate by town, average number of rooms per dwelling, 
Charles River dummy variable (= 1 if tract bounds river; 0 otherwise), 
nitrogen oxides concentration (parts per 10 million) and a constant.

```{r, echo=FALSE}
data1 <- MASS::Boston
X <- as.matrix(cbind(1,data1[,c("crim", "rm","chas","nox")]))
#X <- as.matrix(cbind(1,data1["lstat"]))
colnames(X) <- c("Constant","Crime", "Rooms", "Charles_River", "NO_pp10m")
Y <- as.matrix(data1[,"medv"])
```

Below we wrote a function which computes:
OLS point estimates for the intercept, slope parameters, and the error variance.
Suitable test statistics with corresponding p-values for the relevant coefficients.
Intervals of the coefficients for a confidence level of 95%.

```{r}
OLS <- function(X,Y){

# OLS estimates for coefficients
beta_hat <- solve(t(X) %*% X) %*% t(X) %*% Y  
Y_hat <- X %*% beta_hat   # Fitted values

e <- Y - Y_hat            # residuals
n <- nrow(X)              # Number of observations
k <- ncol(X) - 1          # Number of covariates excluding intercept

s <- as.numeric(t(e)%*%e / (n-k))           # SSR adjusted for degrees of freedom
sigma <- s*solve(t(X) %*% X)                # VCV of Beta hat

se <- sqrt(diag(sigma))                     # standard error
t_stat <- (beta_hat-0) / se                 # Compute t-statistic
p <- pt(abs(t_stat), n-k, lower.tail=FALSE) # Compute p-value

# 95% Confidence interval
th <- qt(0.975, n-k)
conf <- cbind(beta_hat-th*se,beta_hat+th*se) 

colnames(beta_hat) <- "estimate"
colnames(conf) <- c("2.5%","97.5%")
colnames(t_stat) <- "t-statistic"
colnames(p) <- "p-value"

error_variance <- s

list(rbind(beta_hat,error_variance), cbind(t_stat, p), conf)
} 
```

The output of the function is presented below:
```{r}
OLS(X,Y)
```

## Exercise B
1. Draw a graph of the network; create the adjacency matrix in R.

```{r, echo=FALSE}
agents <- c("US", "MX", "DE", "AT", "CN", "IN")

adj <- matrix(c(
    0, 1, 0, 0, 1, 0,
    1, 0, 0, 0, 1, 0,
    1, 0, 0, 0, 0, 0,
    1, 0, 1, 0, 1, 0,
    1, 0, 1, 0, 0, 0,
    1, 0, 0, 0, 1, 0
), nrow = 6, byrow = TRUE)

colnames(adj) = rownames(adj) = agents
```

We could consider 6 different countries and an indicator of a high trade between them. For illustration, we could consider a rule where there is directed edge from A to B, if B is one of the top 5 export destinations of A. We don't actually check for biggest trading partners empirically, but lets assume that such procedure gives rise to following network (arrows are mostly made up to achieve 10 edges and nice interpretations):

```{r, echo=FALSE}
plot(graph_from_adjacency_matrix(adj),vertex.size = 30, main="Trade network",edge.arrow.size = 0.7)
```

The corresponding adjacency matrix is:

```{r, echo=FALSE}
print(adj)
```

2. Who are the most and least central agents in the network? Name, explain, and try
to quantify different notions of centrality.

A very basic concept of centrality could define an agent as most central, if it has the highest number of directed edges pointing towards itself (i.e. if it is important export country for most other countries). Using this criterion we can see from the graph that or from columns of adjacency matrix that US the most central agents in this network, because it is top 5 trading partner for all 5 other countries. The least central agents would be Austria and India, since they are not a top 5 export country for any country from this network.

Another basic criterion of centrality could be the amount of outwards pointing arrows of an agent. Thus a country would be considered central if it exports to highest number of countries from this network. In this sense, Germany is least central with only 1 outward arrow, while Austria is most central with 3 outward arrows.

Eigenvector centrality: There is no sink, but not possible to get to Austria. ?
Page Rank: weights (probs) need to be defined?




• How would centralities change if you considered a row-normalized network
instead?


```{r, echo=FALSE}
scale <- function(x){
  x/sum(x)
}
adj.rn <- t(apply(adj,1,scale))
```

Germany which had only 1 connection, is the only row with sum of 1. Thus its the only edge which survives row normalization. Thus here depending on criterion US or Germany would be most central agent. Other countries would be all least central.

```{r, echo=FALSE}
plot(graph_from_adjacency_matrix(adj.rn),vertex.size = 30, main="Trade network row normalized",edge.arrow.size = 0.7)
```

• How would the network change if you removed or added a specific agent?
Lets consider the case of removing US:

```{r, echo=FALSE}
#agents[-1]
plot(graph_from_adjacency_matrix(adj[-1,-1]),vertex.size = 30, main="Trade network excl. US",edge.arrow.size = 0.7)
```

Based on Inflowing arrows, China becomes the most central agent and India, Austria and Mexico the least central agents. 
Based on Outflowing arrows, Austria is still the most central agent, while Germany is the least central agent.

3.Simulate some agent charaacteristic based on a standard Normal distribution; use this characteristic to simulate responses following a liner-in-means model with your network. 

• Repeat this a couple of times, and compare estimates of a standard linear model ($y_{i} = x_{\beta} + \varepsilon_{i}$) with the true values that you used to simulate the data

```{r, echo = FALSE}
# number of agents
N = 6

# parameters definition
sigma2 = 0.1
lambda = 0.9
beta = 2 # true beta is 2
W = adj.rn

simulations <- 10000
times <- c(1:simulations)
result <- numeric(simulations) 

for (i in times) {

# simulation of vecotrs
x = rnorm(N, 0, 1)
e = rnorm(N, 0, sigma2)

# sampling from reduced form to generate y's

# caluclating means
Wx <- W %*% x

#Calculating S
S = diag(N) - lambda * W

# create y's
y = solve(S,Wx * lambda + x * beta + e)
  
model <- lm(y ~ 1 + x)

result[i] <- coef(model)["x"]

inconsistent_estimate <- mean(result)
} 

my_data <- data.frame(
  Name =  c("Simulated coef", "Real coef"), # Creating a column named ID with values from 1 to 4
  Estimate = c(inconsistent_estimate, beta) # Creating a Name column
)

print(my_data)
```

We simulate the data based on the row normalized adjacency matrix we created in the preceding exercise. To create the values for y, we need to solve for the reduced form. We set the treu value for $\beta$ to 2 and $\lambda$ to 0.9. We then regress y on x, repeat this a 1000 times, and find that the mean of the estimand is 1.503493. Thus using the simple linear regression fails to recover the true value for $\beta$. 

Note that the parameter $\lambda$ plays an interesting role in moderating the size of the bias. The closer $\lambda$ is to zero, the smaller the bias. Conversely, in our case where we set $\lambda$ close to one, the bias is amplified quite strongly.

## Exercise C
Download a suitable shapefile for NUTS2 regions (from here or using R directly) and some
dataset of interest at the same level of aggregation (e.g. from here). Install and load the
sf and ggplot2 packages together with their dependencies.

```{r, include=FALSE}
library(sf)
library(ggplot2)

# create a new empty object called 'temp' in which to store a zip file
# containing boundary data
temp <- tempfile(fileext = ".zip")
download.file("http://ec.europa.eu/eurostat/cache/GISCO/distribution/v2/nuts/download/ref-nuts-2021-03m.shp.zip", temp)

outDir <- "./data"
unzip(temp, exdir=outDir)

#list.files(path="./data", pattern = '*.shp')

# let us choose projection WGS 84 (EPSG 4326) which is visible the file name 
# between the year (2021) and the level of the data (NUTS 2):
unzip("./data/NUTS_RG_03M_2021_4326_LEVL_2.shp.zip", exdir=outDir)

```


• Read in the shapefile, and find out what projection and CRS the file uses.

```{r, include=FALSE}
shp <- st_read(dsn = "./data", layer ="NUTS_RG_03M_2021_4326_LEVL_2") #reads in the shapefile
# The shapefile looks just like a normal dataframe with an additional "geometry" column:
#head(shp)
# Save crs for later
old_crs <- st_crs(shp)
#st_geometry(shp)
```

The projection used is WGS 84 (EPSG 4326).

Map the data to use another projection and/or CRS of your choosing:
Since exclude everything except Austria, we decided for MGI Lambert Austria (equidistant equal-area specifically for AT).

```{r}
# we exclude everything except Austria
shp <- shp[shp$CNTR_CODE %in% "AT", ]
#Therefore we decide for MGI Lambert Austria (equidistant equal-area specifically for AT)
#plot(st_geometry(shp))
st_geometry(shp) <- st_geometry(st_transform(shp, 
                 st_crs("epsg:31287")))
#plot(st_geometry(shp))
```



## Exercise D
```{r, include=FALSE}
komorowskiPalette <- brewer.pal(n = 7, name = "Blues")
dudaPalette <- brewer.pal(n = 7, name = "Reds")

# Corrected tm_polygons function call
tm_shape(pol_pres15) +
  tm_facets(free.scales = TRUE) +  # Enables independent scaling of facets
  tm_polygons(c("II_Komorowski_share", "II_Duda_share"), title = "2. Round Vote Share",
              palette=list("II_Komorowski_share" = komorowskiPalette,
                           "II_Duda_share" = dudaPalette),
              midpoint=0.5) +  # Correct placement of midpoint argument
  tm_layout(legend.position = c("central", "top"))

```




Another way: 
Install and load the tmap and spDataLarge packages (available from GitHub). Load and review the pol_pres15 dataset on the Polish Presidential election in 2015 (see ?pol_pres15). Create three different, insightful visualizations of the underlying data.

• One visualization should compare the support for Komorowski and Duda.

# Combine support for Komorowski and Duda into a single categorical variable
pol_pres15$Support <- ifelse(pol_pres15$I_Komorowski_share > pol_pres15$I_Duda_share, "Komorowski", "Duda")

# Visualize support for Komorowski and Duda in the first run
tm_shape(pol_pres15) + 
  tm_borders(lwd = 0.5, alpha = 0.4) + 
  tm_fill(col = "Support", style = "cat", palette = "-RdYlBu", title = "Support for Candidates") +
  tm_layout(legend.position = c("left", "bottom"))

• One visualization should investigate possible issues with postal voting envelopes.
We want to investigate if Issues occured because voting envelopes got lost on the way to the voters.

# Visualization investigating possible issues with postal voting envelopes
# Calculate the proportion of postal voting envelopes received to postal voting packages sent
pol_pres15$Envelope_Proportion <- pol_pres15$I_postal_voting_envelopes_received / pol_pres15$I_voters_sent_postal_voting_package

# Define a threshold for the proportion indicating possible issues
threshold <- 0.99  # You can adjust this threshold as needed

# Create a new column indicating possible issues with postal voting envelopes based on the proportion
pol_pres15$Envelope_Issues <- ifelse(pol_pres15$Envelope_Proportion < threshold, "Possible Issues", "No Issues")

tm_shape(pol_pres15) + 
  tm_borders(lwd = 0.5, alpha = 0.4) + 
  tm_fill(col = "Envelope_Issues", style = "cat", palette = c("green", "red"), 
          title = "Possible Issues with Postal Voting Envelopes") +
  tm_layout(legend.position = c("left", "bottom"))


• Third vizualization
# Visualize support for Komorowski and Duda in the second run
pol_pres15$Support <- ifelse(pol_pres15$II_Komorowski_share > pol_pres15$II_Duda_share, "Komorowski", "Duda")
tm_shape(pol_pres15) + 
  tm_borders(lwd = 0.5, alpha = 0.4) + 
  tm_fill(col = "Support", style = "cat", palette = "-RdYlBu", title = "Support for Candidates") +
  tm_layout(legend.position = c("central", "bottom"))